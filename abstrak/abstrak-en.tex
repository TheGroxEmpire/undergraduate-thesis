\begin{center}
  \large\textbf{ABSTRACT}
\end{center}

\addcontentsline{toc}{chapter}{ABSTRACT}

\vspace{2ex}

\begingroup
  % Menghilangkan padding
  \setlength{\tabcolsep}{0pt}

  \noindent
  \begin{tabularx}{\textwidth}{l >{\centering}m{3em} X}
    % Ubah kalimat berikut dengan nama mahasiswa
    \emph{Name}     &:& Dafa Fidini Asqav \\

    % Ubah kalimat berikut dengan judul tugas akhir dalam Bahasa Inggris
    \emph{Title}    &:& \emph{Deep Reinforcement Learning Implementation on Hexagonal Grid Turn-Based Strategy Game} \\

    % Ubah kalimat-kalimat berikut dengan nama-nama dosen pembimbing
    \emph{Advisors} &:& 1. Dr. Supeno Mardi Susiki Nugroho, S.T., M.T. \\
                    & & 2. Mochamad Hariadi, ST., M.Sc., Ph.D. \\
  \end{tabularx}
\endgroup

% Ubah paragraf berikut dengan abstrak dari tugas akhir dalam Bahasa Inggris
Strategy games are games where the player takes strategic decision in the game to finish an objective.
One of these games is Civilization 6 (Civ6).
In this game, the players take turn in action in a map area made of hexagonal tiles (hexagonal grid).
Civ6 is also a 4X game. These 4X aspects make the game rather complicated.
This has become a challenge for game developers to create AI opponents that are capable to give enough challenge for the players.
However, strategy games like Civ6 are still having less than optimal AI agent capability.
The advancement in Deep Reinforcement Learning (DRL) allows AI technology that was not possible before.
In this research, an environment that follows the combat mechanics in Civ6 is devised as the
DRL implementation media. There are two agents in this environment: attacker and defender.
Both agent has differing objectives (asymmetrical) and oppose each other (adversarial).
There are four state of the art (SOTA) algorithms used in this research experiment: DQN, APE-X DQN,
PPO, and IMPALA.
From the experiment result, APE-X DQN performed the best for both attacker agent and defender agent.
Attacker agent with APE-X DQN was able to consistently destroying the city before 2 million environment steps.
However, APE-X DQN required higher CPU and RAM utilization than other algorithms,
with 79.95\% CPU utilization and 82.3\% RAM utilization.

% Ubah kata-kata berikut dengan kata kunci dari tugas akhir dalam Bahasa Inggris
\emph{Keywords}: \emph{Deep Reinforcement Learning, Artificial Intelligence, Machine Learning, Civilization VI, DQN, APE-X, PPO, IMPALA}
