\chapter{PENUTUP}
\label{chap:penutup}

% Ubah bagian-bagian berikut dengan isi dari penutup

\section{Kesimpulan}
\label{sec:kesimpulan}

Dalam penelitian telah diimplementasikan beberapa algoritma \emph{state of the art} \emph{deep reinforcement learning} dalam \emph{environment}
yang mengikuti mekanisme \emph{combat} dalam \emph{Civilization VI}.
Dalam \emph{environment} ini, terdapat dua agen: agen \emph{attacker} dan agen \emph{defender}.
Kedua agen mempunyai tujuan yang berbeda \emph{asymmetrical} dan saling berlawanan \emph{adversarial}.
Dalam eksperimen dilakukan \emph{training} pada setiap algoritma selama 100 juta \emph{environment steps}.
Dilakukan juga evaluasi terhadap seluruh generasi dari bobot hasil \emph{training}.
Terdapat dua evaluasi, evaluasi antar generasi dan evaluasi dengan \emph{environment} yang lebih luas dari \emph{environment} saat training.
Dari eksperimen yang dilakukan dengan algoritma-algoritma tersebut, didapatkan:

\begin{enumerate}[nolistsep]

  \item \emph{Deep reinforcement learning} dapat digunakan untuk membuat agen yang cukup dalam \emph{environment} yang mensimulasi mekanisme \emph{combat}
  Civ6 secara terbatas.

  \item \emph{Distributed Prioritized Experience Replay Deep Q-Networks} merupakan algoritma yang memiliki reward paling tinggi saat \emph{training}. Agen \emph{attacker} dengan \emph{Distributed Prioritized Experience Replay Deep Q-Networks} mampu
  menghancurkan kota secara konsisten sebelum 2 juta \emph{environment steps}.
  Agen \emph{defender} dengan \emph{Distributed Prioritized Experience Replay Deep Q-Networks} juga merupakan algoritma yang paling optimal
  dibanding algoritma lain saat \emph{training}.

  \item Saat training \emph{Distributed Prioritized Experience Replay Deep Q-Networks} menggunakan RAM dan CPU terbanyak dengan penggunaan CPU sebanyak 79.95\% dan penggunaan
  RAM sebanyak 82.3\%.

  \item Pada evaluasi \emph{Distributed Prioritized Experience Replay Deep Q-Networks} dan \emph{Proximal Policy Optimization} memiliki masalah dalam inferensi.
  Sehingga evaluasi tidak dilakukan secara penuh pada setiap generasi. Hal ini memungkinkan masalah yang sama terjadi ketika model digunakan dalam sebuah environment lain.

  \item Terdapat banyak tantangan dalam mengimplementasikan agen DRL dalam \emph{environment hexagonal grid turn based strategy games}.
  Tantangan berupa kesulitan dalam \emph{debugging}, jarangnya contoh reproduksi kode DRL, sulitnya mencari reward yang tepat,
  \emph{learning curve} dalam memahami DRL yang masih tinggi, kerumitan dalam melakukan evaluasi performa agen,
  dan ketidakstabilan agen DRL. 
\end{enumerate}

\section{Saran}
\label{chap:saran}

Dalam penelitian ini, ada beberapa aspek yang dapat diperbaiki untuk kedepannya:

\begin{enumerate}[nolistsep]

  \item Menambahkan tipe \emph{unit} yang lebih banyak dalam \emph{environment}: \emph{siege}, \emph{range}
  dengan jarak lebih jauh, \emph{cavalry}, dan \emph{recon}.

  \item Mengimplementasikan \emph{randomly generated terrain} pada \emph{environment} sehingga agen lebih
  mampu beradaptasi.

  \item Mengimplementasikan pemilihan \emph{unit} untuk agen yang lebih fleksibel dari pemilihan sekuensial
  yang digunakan sekarang, seperti menggunakan \emph{pointer net}.

  \item Melakukan hyperparameter tuning pada algoritma yang diuji coba.
  
  \item Mengintegrasikan algoritma ke dalam game engine seperti Unity3D.

\end{enumerate}
