\chapter{TINJAUAN PUSTAKA}
\label{chap:tinjauanpustaka}

% Ubah bagian-bagian berikut dengan isi dari tinjauan pustaka

\section{Penelitian Terdahulu}
\label{sec:penelitianterdahulu}

Penelitian-penelitian sebelumnya menggunakan beberapa metode AI untuk menyelesaikan masalah ini. Sebagian besar dari penelitian-penelitian sebelumnya berusaha untuk memperbaiki algoritma AI di game strategi seperti seri Civilization dalam sisi pembangunan kota atau ekonomi dalam game strategi. Namun, dalam sisi combat untuk game strategi masih belum diteliti.

I. Athaillah, S. M. S. Nugroho, dan M. Hariadi menggunakan Non-dominated Sorting Genetic Algorithm II (NGSA II) untuk optimasi pemilihan distrik kota dalam game Civilization VI. Dibandingkan dengan metode AI yang ada dalam game tersebut, agen AI dalam simulasi penelitian ini dapat memberikan pemilihan distrik kota yang lebih optimal. Penelitian yang dilakukan oleh I. Athaillah, et al hanya berfokus pada pemilihan distrik dalam sebuah kota. Pemilihan distrik berpengaruh terhadap perkembangan ekonomi pemain. Metode NGSA II yang digunakan kurang cukup untuk mengatasi permasalahan dalam combat untuk AI. 

S. Wender dan I. Watson, menggunakan metode \emph{Q-Learning} untuk pemilihan bangunan kota dalam game Civilization IV. Performa agen AI dalam penelitian ini didapatkan lebih baik dibandingkan agen AI dalam game tersebut.

C. Amato dan G. Shani juga menggunakan sebuah metode \emph{Q-Learning} berupa \emph{Dyna-Q} dalam Civilization IV. Agen dalam penelitian ini didapatkan dapat beradaptasi terhadap lawan agen AI yang menggunakan \emph{fixed policy}.

Metode Reinforcement Learning yang digunakan oleh C. Amato dan G. Shani \citep{civ4RL}, serta S. Wender dan I. Watson kami rasa mencukupi untuk penelitian ini. Akan tetapi, game strategi Civilization IV yang dijadikan objek penelitian menggunakan desain game yang tidak lagi umum digunakan. Civilization IV menggunakan rectangular grid dibandingkan hexagonal grid yang banyak digunakan oleh game-game strategi berbasis giliran saat ini. Di dalam Civilization IV, tidak terdapat pembatasan \emph{unit} per tile yang banyak digunakan oleh game strategi saat ini. Tanpa batasan ini, pemain dapat melakukan doomstacking dimana pemain dapat mengelompokan pasukan dalam satu tile sebanyak-banyaknya. Adanya doomstacking membatasi pilihan strategi yang optimal dalam combat.

A. Sestini, et al \citep{deepCrawl} mengembangkan sebuah game bernama DeepCrawl yang menggunakan Deep Reinforcement Learning (DRL) sebagai metode untuk mengembangkan AI dalam game tersebut. AI yang dilatih dalam penelitian ini dapat memberikan tantangan yang cukup bagi banyak pemain.[5] Penggunaan DRL seperti yang digunakan oleh A. Sestini, et al pada game DeepCrawl merupakan metode yang paling optimal dalam game strategi. Penggunaan Neural Network (NN) dapat diskala untuk area game yang besar. Namun, DeepCrawl merupakan jenis game Roguelike yang sangat berbeda dan minim dibandingkan dengan game strategi 4X.

\section{Dasar Mekanisme dalam Combat Civilization VI}
\emph{Civilization VI} menggunakan \emph{hexagonal grid} yang terdiri dari \emph{hex tiles}, atau \emph{hexes}, sebagai basis pergerakan dan peletakan \emph{unit}. 
\emph{Unit} dapat bergerak ke enam arah menuju \emph{hexes} yang terdekat. Setiap \emph{unit} memiliki \emph{movement point} (MP). 
MP menentukan seberapa jauh \emph{unit} dapat bergerak atau melakukan aksi dalam satu giliran. 
Setiap pergerakan menuju \emph{hexes} lain akan mengurangi MP yang dimiliki \emph{unit} tersebut.
\emph{Unit} akan mendapatkan seluruh MP kembali setelah gilirannya selesai.

Saat \emph{combat}, \emph{unit} dapat memakan seluruh MP yang dimilikinya saat itu untuk menyerang \emph{unit} lain.
\emph{Unit} juga memiliki \emph{hit point} (HP), \emph{combat strength (CS)}, \emph{ranged strength} (RS), dan \emph{bombard strength} (BS).
CS digunakan ketika \emph{unit} menyerang \emph{unit} lain secara dekat, RS digunakan ketika \emph{unit} menyerang \emph{unit} lain menggunakan senjata jarak jauh, dan BS digunakan oleh \emph{siege} \emph{unit} ketika menyerang pertahanan kota.
\emph{Unit} yang diserang atau menyerang dengan CS akan mendapatkan \emph{damage} mengikuti persamaan eksponential\citep{civ6Combat}:

\[Damage(HP) = 30 * e^{(0.04+StrengthDifference)} * randomBetween(75\%, 125\%)\]

\emph{Unit} yang menerima \emph{damage} akan mendapatkan CS \emph{penalty} sebesar \(10 - HP/10\).
\emph{Unit} dapat mendapatkan kembali HP yang hilang (\emph{healing}) dengan cara tidak menggunakan MP yang dimiliki selama gilirannya.
\emph{Unit} yang melakukan \emph{healing} mendapatkan +10 HP per giliran.

% input gambar
\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.6]{gambar/hex_action.png}
    % Keterangan gambar yang diinputkan
    \caption{7 \emph{actions} yang dapat diambil oleh AI dalam \emph{Hexagonal tile}}
    % Label referensi dari gambar yang diinputkan
    \label{fig:hex_action}
\end{figure}

\section{Machine Learning}

\emph{Machine Learning} secara definisi merupakan suatu bidang ilmu komputer yang berkembang dari studi pengenalan pola dan teori pembelajaran komputasional dalam kecerdasan buatan. 
Prediksi dataset \emph{Machine Learning} dibangun dari sebuah model yang mendapatkan contoh input untuk membuat prediksi berbasis data daripada mengikuti sebuah instruksi program statis \citep{machineL}.

\section{Deep Learning}
\emph{Deep Learning} merupakan bentuk dari \emph{machine learning} yang memungkinkan komputer untuk mempelajari pengalamannya dan memahami dunia secara konsep hierarkis. 
Konsep hierarkis ini memungkinkan komputer untuk mempelajari konsep rumit dengan membangun konsep tersebut dari konsep-konsep yang lebih sederhana \citep{deepL}.

\section{Deep Reinforcement Learning}
\emph{Deep Reinforcement Learning} (DRL) merupakan jenis dari \emph{Reinforcement Learning} (RL), dengan \emph{deep neural network} sebagai representasi state
dan/atau fungsi pendekatan untuk fungsi \emph{value}, \emph{policy}, \emph{transition model}, atau fungsi \emph{reward} \citep{deepRL}.
Sebuah environment game umum digunakan sebagai bidang eksperimentasi DRL.
\emph{DeepMind} membuat sebuah model DRL (\emph{AlphaStar}) yang dapat mengalahkan pemain professional dalam game \emph{Starcraft 2}, membuktikan bahwa DRL dapat digunakan untuk membangun agen AI yang mumpuni dalam sebuah game strategi \citep{alphaStar}.
\emph{OpenAI} juga mempresentasikan sebuah model yang dapat mengalahkan pemain professional dalam sebuah game \emph{Dota 2} \citep{openaiDota2}.
Penelitian-penelitian tersebut telah membuktikan kemampuan DRL dalam membuat sebuah agen AI yang mumpuni dalam game strategi.

\section{Deep Q-Learning Network}
Mnih, et al mengenalkan \emph{Deep Q-Learning Network} (DQN)\citep{deepQN} sebagai pencetus pertama metode \emph{Deep Reinforcement Learning}. DQN menggunakan \emph{neural network} untuk memperkirakan nilai optimal dari fungsi \emph{action value} pada \emph{Q-Learning}.
DQN, menggunakan \emph{experience replay} untuk mengatasi ketidakstabilan dan divergensi yang muncul pada \emph{Q-Learning} seperti, \emph{off-policy, function approximation,} dan \emph{bootstraping} \citep{deepQNFunction}.

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.35]{gambar/dqn_algorithm.png}
    % Keterangan gambar yang diinputkan
    \caption{Algorithma \emph{Deep Q-Learning}}
    % Label referensi dari gambar yang diinputkan
    \label{fig:dqnAlgorithm}
\end{figure}

DQN memiliki kelemahan dimana perkiraan dari sebuah \emph{action value} nilainya terlalu besar.
DQN juga memiliki tendensi untuk mengejar \emph{action value} yang terbesar walau mungkin aksi tersebut tidaklah optimal pada situasi yang sedang dihadapi.
Beberapa varian DQN mencoba untuk menyelesaikan masalah ini. 

\emph{Double Deep Q-Learning} (DDQN) mencoba menstabilkan nilai \emph{action value} yang terlalu besar dengan menggunakan dua cabang \emph{hidden layer} pada DQN \citep{doubleDQN}.

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.2]{gambar/ddqn_architecture.png}
    % Keterangan gambar yang diinputkan
    \caption{Arsitektur model algoritma \emph{Double Deep Q-Learning}}
    % Label referensi dari gambar yang diinputkan
    \label{fig:ddqnArchitecture}
\end{figure}

\emph{Prioritized Experience Replay} (PER) meluaskan fungsi \emph{experience replay function} dengan mempelajari \emph{replay memories} yang memiliki nilai divergensi \emph{reward} yang nyata dengan yang diharapkan.\citep{prioritizedER}

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.3]{gambar/ddqn_per_algorithm.png}
    % Keterangan gambar yang diinputkan
    \caption{Algorithma \emph{Double Deep Q-Learning} dengan \emph{Prioritized Experience Replay}}
    % Label referensi dari gambar yang diinputkan
    \label{fig:dqnPerAlgorithm}
\end{figure}

\emph{Dueling} DQN memisahkan neural network menjadi dua bagian, satu bagian mempelajari dan memberikan estimasi nilai pada seluruh \emph{timestep},
dan bagian lain menghitung nilai potensial (\emph{advantage}) dari sebuah aksi.
Kedua bagian ini digabungkan menjadi satu \emph{action-advantage Q function} \citep{duelingDQN}.

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.35]{gambar/dqn_vs_dueling_dqn_architecture.jpg}
    % Keterangan gambar yang diinputkan
    \caption{Arsitektur DQN (di atas) dibandingkan dengan Arsitektur \emph{Dueling} DQN (di bawah)}
    % Label referensi dari gambar yang diinputkan
    \label{fig:dqnVsDuelingDqnArchitecture}
\end{figure}

\section{Distributed Prioritized Experience Replay Deep Q-Networks (APE-X DQN)}

Distributed Prioritized Experience Replay Deep Q-Networks (Ape-X DQN) merupakan arsitektur DQN terdistribusi.
Dibandingkan dengan DQN biasanya, Ape-X DQN menggunakan banyak \emph{core} pada CPU sebagai \emph{actor} yang masing masing memiliki environment tersendiri.
\emph{Actor} tersebut akan mengumpulkan \emph{experience} yang didapatkan ke sebuah GPU \emph{learner}. 
\emph{Learner} tersebut akan mengambil sebuah sampel dengan metode \emph{prioritized experience} dan melakukan update pada \emph{neural network}. 
Ape-X DQN ditemukan lebih efisien dan lebih baik daripada algoritma DQN lainnya dalam \emph{environment} game \emph{Atari} 2600 \citep{apexDQN}.

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.5]{gambar/apex1.png}
    % Keterangan gambar yang diinputkan
    \caption{Arsitektur Ape-X dengan beberapa \emph{actor} dan menambahkan \emph{experiences} ke pada sebuah \emph{learner}}
    % Label referensi dari gambar yang diinputkan
    \label{fig:apexDQNArchitecture}
\end{figure}

\section{Proximal Policy Optimization}

\emph{Proximal Policy Optimization} (PPO) merupakan sebuah \emph{Policy Gradient Methods}. 
Berbeda dengan \emph{Q-Learning}, \emph{policy gradient methods} bekerja dengan cara menghitung sebuah estimator dari \emph{policy gradient} dan memasukannya ke
sebuah algoritma \emph{stochastic gradient ascent} \citep{ppo}. PPO mengoptimasi lebih lanjut dari mode \emph{Trust Region Policy Optimization} (TRPO).
Jika biasanya metode \emph{policy-gradient} melakukan satu \emph{gradient update} setiap sampel, PPO melakukan beberapa \emph{epochs} dari \emph{minibatch updates}. 
PPO memiliki beberapa keuntungan dari TRPO dengan implementasi yang lebih mudah, lebih baik dalam generalisasi, dan memiliki performa yang lebih baik dalam simulasi permainan game Atari dibandingkan dengan metode policy gradient yang lain

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.5]{gambar/ppo_model.png}
    % Keterangan gambar yang diinputkan
    \caption{Arsitektur algoritma PPO}
    % Label referensi dari gambar yang diinputkan
    \label{fig:ppoArchitecture}
\end{figure}

\section{Importance Weighted Actor-Learner Architecture (IMPALA)}

IMPALA merupakan sebuah \emph{framework} \emph{actor-critic} yang memisahkan antara acting dengan learning.
IMPALA menggunakan \emph{V-trace} dimana \emph{learner} mempelajari \emph{experience trajectory}.
GPU \emph{learner} pusat melakukan \emph{stochastic gradient descent} (SGD) pada sebuah \emph{tight loop} saat mengambil sampel \emph{batches} dari proses CPU \emph{actor} secara sinkronus.

Arsitektur IMPALA dapat mencapai \emph{throughput} yang tinggi, namun \emph{policy} yang digunakan untuk menghasilkan \emph{trajectory} terlambat selama beberapa \emph{update} jika dibandingkan dengan \emph{policy} yang ada pada \emph{learner} pada saat perhitungan gradien.
Oleh karena itu, IMPALA menggunakan \emph{V-trace off policy actor critic}

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.05]{gambar/impala_worker.jpg}
    % Keterangan gambar yang diinputkan
    \caption{Kiri: Sebuah learner. Setiap \emph{actor} menghasilkan \emph{trajectories} dan mengirimkannya melalui sebuah \emph{queue} kepada \emph{learner}.
    Sebelum memulai \emph{trajectory} selanjutnya, \emph{actor} menerima \emph{policy parameters} terbaru dari \emph{learner}.
    Kanan: Banyak \emph{learner}. \emph{Policy parameters} didistribusikan kepada lebih dari satu \emph{learner} yang bekerja secara sinkron}
    % Label referensi dari gambar yang diinputkan
    \label{fig:impalaSingleVsMultipleLearner}
\end{figure}

% % Contoh input gambar
% \begin{figure}[ht]
%   \centering

%   % Ubah dengan nama file gambar dan ukuran yang akan digunakan
%   \includegraphics[scale=0.35]{gambar/roketluarangkasa.jpg}

%   % Ubah dengan keterangan gambar yang diinginkan
%   \caption{Peluncuran roket luar angkasa \emph{Discovery} \citep{roketluarangkasa}.}
%   \label{fig:roketluarangkasa}
% \end{figure}

% Roket luar angkasa merupakan \lipsum[1]

% \emph{Discovery}, Gambar \ref{fig:roketluarangkasa}, merupakan \lipsum[2]

% % Per Teori Penunjang dibuat section baru

% \section{Gravitasi}
% \label{sec:gravitasi}

% Gravitasi merupakan \lipsum[1]

% \subsection{Hukum Newton}
% \label{subsec:hukumnewton2}

% Newton \citep{newton1687} pernah merumuskan bahwa \lipsum[1]
% Kemudian menjadi persamaan seperti pada persamaan \ref{eq:hukumpertamanewton}.

% % Contoh pembuatan persamaan
% \begin{equation}
%   \label{eq:hukumpertamanewton}
%   \sum \mathbf{F} = 0\; \Leftrightarrow\; \frac{\mathrm{d} \mathbf{v} }{\mathrm{d}t} = 0.
% \end{equation}

% \subsection{Anti Gravitasi}
% \label{subsec:antigravitasi}

% Anti gravitasi merupakan \lipsum[1]
