\chapter{HASIL DAN PEMBAHASAN}
\label{chap:hasilpembahasan}

Pada bab ini dipaparkan hasil dari eksperimen yang dilakukan pada penelitian ini.
Hasil dari eksperimen berupa \emph{mean episode rewards} dari agen \emph{attacker} dan agen \emph{defender},
waktu training, rerata waktu inferensi, rerata waktu proses \emph{actions}, rerata waktu \emph{environment} menunggu,
rerata waktu pemrosesan observasi mentah, dan presentase penggunaan CPU serta RAM.

\section{Mean Episode Rewards}

Dalam eksperimen RL, hal yang paling penting untuk diperhatikan adalah rerata \emph{rewards} (\emph{mean episode rewards}) yang didapatkan oleh agen
selama training. Performa agen RL dapat diukur dengan seberapa banyak reward yang telah didapatkan oleh agen.
Dari eksperimen yang telah dilakukan, didapatkan \emph{mean episode rewards} untuk kedua agen dengan keempat algoritma yang diuji coba.

\subsection{Attacker Mean Episode Rewards}

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.65]{gambar/attacker_reward_mean.jpg}
    % Keterangan gambar yang diinputkan
    \caption{Grafik menunjukan \emph{mean episode rewards} dari agen \emph{attacker}.
    Biru: APE-X DQN, oranye: DQN, hijau: IMPALA, merah: PPO}
    % Label referensi dari gambar yang diinputkan
    \label{fig:attackerMeanEpisodeGraph}
\end{figure}

Dari grafik \ref{fig:attackerMeanEpisodeGraph}, APE-X DQN merupakan algoritma yang paling baik,
jauh di atas algoritma lain. APE-X DQN mencapai nilai 20 poin sekitar pada 1.5 \emph{environment steps}.
APE-X DQN berkonvergensi di sekitar nilai 20 poin ini sebelum 2 juta \emph{environment steps}.
Mengingat 20 poin merupakan nilai \emph{reward} ketika agen \emph{attacker} menghancurkan kota,
agen \emph{attacker} APE-X DQN secara konsisten telah menyelesaikan tujuan tersebut.

Algoritma lain jauh dibawah performa APE-X DQN. Algoritma DQN selalu di bawah 5 poin dan tidak meningkat secara signifikan.
Algoritma PPO mencapai nilai 5 poin pada sekitar 1.5 juta \emph{environment steps}, akan tetapi menurun setelah 5 juta
\emph{environment steps}. PPO juga mengalami \emph{flatline} dimana secara tiba tiba algoritma tersebut
tidak mengalami peningkatan atau penurunan nilai \emph{rewards} pada sebelum 8 juta \emph{environment steps}
sampai akhir sesi. PPO merupakan algoritma terburuk pada grafik ini. IMPALA merupakan algoritma yang paling tidak stabil pada grafik ini.
Nilai \emph{mean episode rewards} agen \emph{attacker} IMPALA selalu di bawah -20 poin sampai
sekitar sebelum 8 juta \emph{environment steps} dimana IMPALA melonjak ke nilai 0 poin.
Nilai poin negatif dari IMPALA hanya bisa dicapai jika \emph{unit} agen \emph{attacker} mencoba untuk keluar
dari batas \emph{environment} secara terus menerus.

\subsection{Defender Mean Episode Rewards}

\begin{figure}[H]
  \centering
    % Nama dari file gambar yang diinputkan
    \includegraphics[scale=0.65]{gambar/defender_reward_mean.jpg}
    % Keterangan gambar yang diinputkan
    \caption{Grafik menunjukan \emph{mean episode rewards} dari agen \emph{defender}.
    Biru: APE-X DQN, oranye: DQN, hijau: IMPALA, merah: PPO}
    % Label referensi dari gambar yang diinputkan
    \label{fig:defenderMeanEpisodeGraph}
\end{figure}

Perubahan nilai \emph{rewards} algoritma dalam grafik \ref{fig:defenderMeanEpisodeGraph} jauh lebih stabil.
Hal ini disebabkan karena nilai \emph{general reward} yang diterima oleh agen \emph{defender} tidak memiliki varian yang tinggi,
dibandingkan dengan agen \emph{attacker} yang memerima 20 poin ketika menghancurkan kota.

APE-X DQN memulai dengan -40 poin akan tetapi secara cepat melonjak ke atas 2 poin.
Sama dengan di grafik \ref{fig:attackerMeanEpisodeGraph}, APE-X DQN merupakan algoritma paling stabil.
APE-X DQN juga salah satu algoritma dengan performa terbaik, bersebelahan dengan IMPALA.

DQN tidak berhasil melakukan konvergensi dengan baik dan nilainya tidak stabil sampai akhir sesi.
PPO juga merupakan algoritma terburuk pada grafik ini. Keadaan \emph{flatline} sama yang dialami PPO pada grafik
\ref{fig:attackerMeanEpisodeGraph} juga dapat dilihat disini.

\section{Waktu dan Pemakaian Spesifikasi Algoritma}

\begin{table}[H]
  \centering
  \caption{Training, Inference, Processing Time, and Resource Usage}
  \label{table:timeAndResourceSpent}
  \begin{tabular}{|l|l|l|l|l|}
      \hline
      \textbf{Metrics}                                                            & \textbf{APE-X DQN} & \textbf{DQN}     & \textbf{PPO}     & \textbf{IMPALA}  \\ \hline
      \begin{tabular}[c]{@{}l@{}}Total Training \\ Time\end{tabular}          & 1d 17h             & 8d 7h            & 3d 3h             & 23h     \\ \hline
      \begin{tabular}[c]{@{}l@{}}Mean Inference \\ (ms)\end{tabular}          & 3.406     & 9.191   & 2.446   & 1.862   \\ \hline
      \begin{tabular}[c]{@{}l@{}}Mean Action \\ Processing (ms)\end{tabular}  & 0.06758   & 0.07171 & 0.05702 & 0.05636 \\ \hline
      \begin{tabular}[c]{@{}l@{}}Mean Environment \\ Wait (ms)\end{tabular}   & 0.457     & 0.4237  & 0.414   & 0.4119  \\ \hline
      \begin{tabular}[c]{@{}l@{}}Mean Raw Obs \\ Processing (ms)\end{tabular} & 0.3512    & 0.2834  & 0.2363  & 0.2143  \\ \hline
      RAM Utilization                                                         & 82.3\%    & 38.82\% & 36.1\%  & 33.2\%  \\ \hline
      CPU Utilization                                                         & 79.95\%   & 40.78\% & 39.63\% & 63.51\% \\ \hline
  \end{tabular}
\end{table}

Tabel \ref{table:timeAndResourceSpent} menunjukan bahwa IMPALA merupakan algoritma tercepat
untuk menyelesaikan 10 juta \emph{environment step} dengan \emph{total training time} selama
23 jam. APE-X DQN mengikuti IMPALA tidak jauh setelahnya dengan \emph{total training time}
selama 1 hari 7 jam (30 jam). PPO memerlukan waktu lebih dari 3x lebih lama dengan \emph{total training time}
sebesar 3 hari 3 jam (75 jam). DQN merupakan algoritma terlama yang memerlukan 8 hari 7 jam (199 jam).

IMPALA juga merupakan algoritma tercepat dalam inferensi dengan nilai \emph{mean inference} sebesar
1.862 ms. PPO merupakan algoritma tercepat kedua dengan \emph{mean inference} sebesar 2.446 ms walaupun \emph{total
training time} PPO lebih lambat dari APE-X DQN, APE-X DQN berada di setelah PPO dengan \emph{mean inference}
sebesar 3.406 ms. DQN merupakan algoritma terlama dalam melakukan inferensi dengan 9.191 ms.
\emph{Metrics} lainnya seperti \emph{mean action processing}, \emph{mean environment wait}, dan \emph{mean raw obs processing}
tidak berbeda jauh antara satu algoritma dengan algoritma lainnya.

Pada penggunaan RAM dan CPU, APE-X DQN merupakan algoritma yang menggunakan CPU dan RAM terbanyak
dengan penggunaan CPU sebesar 79.95\% dan RAM sebesar 82.3\%. Hal ini disebabkan oleh penggunaan CPU
APE-X DQN yang menggunakan seluruh 8 \emph{cores} dibandingkan algoritma lain yang menggunakan 4 \emph{cores}
dengan jumlah \emph{workers} yang sama (3 \emph{workers}). IMPALA merupakan algoritma DRL
terdistribusi lainnya yang menggunakan CPU terbanyak setelah APE-X DQN dengan penggunaan sebesar
63.51\%. Akan tetapi penggunaan RAM dari IMPALA jauh lebih minim dengan penggunaan sebesar 33.2\%.
PPO dan DQN menggunakan CPU dan RAM sama banyaknya dengan penggunaan CPU sebesar 39-40\% dan RAM
sebesar 36-39\%.